{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "259bf032",
   "metadata": {
    "papermill": {
     "duration": 0.0211,
     "end_time": "2022-03-14T05:20:28.068082",
     "exception": false,
     "start_time": "2022-03-14T05:20:28.046982",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# üìñ Feedback - Baselineü§ó Sentence Classifier [0.226]\n",
    "\n",
    "![](https://storage.googleapis.com/kaggle-competitions/kaggle/31779/logos/header.png)\n",
    "\n",
    "###  Starter noteboook for the competition [Feedback Prize - Evaluating Student Writing](https://www.kaggle.com/c/feedback-prize-2021/) framing the task as a sentence classification problem, using HuggingFace, BERT, and the Trainer API, with a LB of `0.226`.\n",
    "\n",
    "\n",
    "#### A lot of competitions going on, right my friends? ;)\n",
    "\n",
    "### The nature of this task is not trivially easy to map to an \"orthodox\" NLP problem, at least as far as I can tell. Is it a Token classification as NER/POS? Is it a... hindi bilingual Question Answering (lol)?\n",
    "\n",
    "<h2> In this notebook I will try to present one of the possible approaches, this is: <span style=\"color:blue\"> Sentence Classification</span>.</h2>\n",
    "\n",
    "---\n",
    "\n",
    "The agenda is as follows:\n",
    "1. A very quick EDA\n",
    "2. Preprocess to obtain a sentence classification dataset\n",
    "3. Fine-tune a BERT over that sentence classification dataset\n",
    "4. Submit\n",
    "\n",
    "---\n",
    "\n",
    "## Please, _DO_ upvote if you find it useful or interesting!! \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43e2289e",
   "metadata": {
    "papermill": {
     "duration": 0.019925,
     "end_time": "2022-03-14T05:20:28.110299",
     "exception": false,
     "start_time": "2022-03-14T05:20:28.090374",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b63d4082",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-14T05:20:28.155415Z",
     "iopub.status.busy": "2022-03-14T05:20:28.153877Z",
     "iopub.status.idle": "2022-03-14T05:20:36.141612Z",
     "shell.execute_reply": "2022-03-14T05:20:36.140933Z",
     "shell.execute_reply.started": "2021-12-20T18:51:10.45136Z"
    },
    "papermill": {
     "duration": 8.011631,
     "end_time": "2022-03-14T05:20:36.141777",
     "exception": false,
     "start_time": "2022-03-14T05:20:28.130146",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from datasets import Dataset\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from transformers import TrainingArguments, Trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd70d81b",
   "metadata": {
    "papermill": {
     "duration": 0.023166,
     "end_time": "2022-03-14T05:20:36.187627",
     "exception": false,
     "start_time": "2022-03-14T05:20:36.164461",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 10,000 foot view of the data\n",
    "\n",
    "Let's take a quick glance at the `train.csv` file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c5cfffb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-14T05:20:36.238154Z",
     "iopub.status.busy": "2022-03-14T05:20:36.234060Z",
     "iopub.status.idle": "2022-03-14T05:20:37.895859Z",
     "shell.execute_reply": "2022-03-14T05:20:37.896293Z",
     "shell.execute_reply.started": "2021-12-20T18:51:19.029508Z"
    },
    "papermill": {
     "duration": 1.686877,
     "end_time": "2022-03-14T05:20:37.896439",
     "exception": false,
     "start_time": "2022-03-14T05:20:36.209562",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>discourse_id</th>\n",
       "      <th>discourse_start</th>\n",
       "      <th>discourse_end</th>\n",
       "      <th>discourse_text</th>\n",
       "      <th>discourse_type</th>\n",
       "      <th>discourse_type_num</th>\n",
       "      <th>predictionstring</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>423A1CA112E2</td>\n",
       "      <td>1622627660524</td>\n",
       "      <td>8</td>\n",
       "      <td>229</td>\n",
       "      <td>Modern humans today are always on their phone....</td>\n",
       "      <td>Lead</td>\n",
       "      <td>Lead 1</td>\n",
       "      <td>1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>423A1CA112E2</td>\n",
       "      <td>1622627653021</td>\n",
       "      <td>230</td>\n",
       "      <td>312</td>\n",
       "      <td>They are some really bad consequences when stu...</td>\n",
       "      <td>Position</td>\n",
       "      <td>Position 1</td>\n",
       "      <td>45 46 47 48 49 50 51 52 53 54 55 56 57 58 59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>423A1CA112E2</td>\n",
       "      <td>1622627671020</td>\n",
       "      <td>313</td>\n",
       "      <td>401</td>\n",
       "      <td>Some certain areas in the United States ban ph...</td>\n",
       "      <td>Evidence</td>\n",
       "      <td>Evidence 1</td>\n",
       "      <td>60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>423A1CA112E2</td>\n",
       "      <td>1622627696365</td>\n",
       "      <td>402</td>\n",
       "      <td>758</td>\n",
       "      <td>When people have phones, they know about certa...</td>\n",
       "      <td>Evidence</td>\n",
       "      <td>Evidence 2</td>\n",
       "      <td>76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 9...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>423A1CA112E2</td>\n",
       "      <td>1622627759780</td>\n",
       "      <td>759</td>\n",
       "      <td>886</td>\n",
       "      <td>Driving is one of the way how to get around. P...</td>\n",
       "      <td>Claim</td>\n",
       "      <td>Claim 1</td>\n",
       "      <td>139 140 141 142 143 144 145 146 147 148 149 15...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             id   discourse_id  discourse_start  discourse_end  \\\n",
       "0  423A1CA112E2  1622627660524                8            229   \n",
       "1  423A1CA112E2  1622627653021              230            312   \n",
       "2  423A1CA112E2  1622627671020              313            401   \n",
       "3  423A1CA112E2  1622627696365              402            758   \n",
       "4  423A1CA112E2  1622627759780              759            886   \n",
       "\n",
       "                                      discourse_text discourse_type  \\\n",
       "0  Modern humans today are always on their phone....           Lead   \n",
       "1  They are some really bad consequences when stu...       Position   \n",
       "2  Some certain areas in the United States ban ph...       Evidence   \n",
       "3  When people have phones, they know about certa...       Evidence   \n",
       "4  Driving is one of the way how to get around. P...          Claim   \n",
       "\n",
       "  discourse_type_num                                   predictionstring  \n",
       "0             Lead 1  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 1...  \n",
       "1         Position 1       45 46 47 48 49 50 51 52 53 54 55 56 57 58 59  \n",
       "2         Evidence 1    60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75  \n",
       "3         Evidence 2  76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 9...  \n",
       "4            Claim 1  139 140 141 142 143 144 145 146 147 148 149 15...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Constants\n",
    "TRAIN_CSV = \"../input/feedback-prize-2021/train.csv\"\n",
    "SUB_CSV = \"../input/feedback-prize-2021/sample_submission.csv\"\n",
    "TRAIN_PATH = \"../input/feedback-prize-2021/train\"\n",
    "TEST_PATH = \"../input/feedback-prize-2021/test\"\n",
    "\n",
    "# Load DF\n",
    "df = pd.read_csv(TRAIN_CSV, dtype={'discourse_id': int, 'discourse_start': int, 'discourse_end': int})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fdff274",
   "metadata": {
    "papermill": {
     "duration": 0.019933,
     "end_time": "2022-03-14T05:20:37.937233",
     "exception": false,
     "start_time": "2022-03-14T05:20:37.917300",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "From the [Data tab](https://www.kaggle.com/c/feedback-prize-2021/data):\n",
    "\n",
    "* id - ID code for essay response\n",
    "* discourse_id - ID code for discourse element\n",
    "* discourse_start - character position where discourse element begins in the essay response\n",
    "* discourse_end - character position where discourse element ends in the essay response\n",
    "* discourse_text - text of discourse element\n",
    "* discourse_type - classification of discourse element\n",
    "* discourse_type_num - enumerated class label of discourse element\n",
    "* predictionstring - the word indices of the training sample, as required for predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "494225df",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-14T05:20:37.980308Z",
     "iopub.status.busy": "2022-03-14T05:20:37.979515Z",
     "iopub.status.idle": "2022-03-14T05:20:38.062095Z",
     "shell.execute_reply": "2022-03-14T05:20:38.062472Z",
     "shell.execute_reply.started": "2021-12-20T18:51:21.113702Z"
    },
    "papermill": {
     "duration": 0.1053,
     "end_time": "2022-03-14T05:20:38.062610",
     "exception": false,
     "start_time": "2022-03-14T05:20:37.957310",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                    0\n",
       "discourse_id          0\n",
       "discourse_start       0\n",
       "discourse_end         0\n",
       "discourse_text        0\n",
       "discourse_type        0\n",
       "discourse_type_num    0\n",
       "predictionstring      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# No nulls\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3c64031",
   "metadata": {
    "papermill": {
     "duration": 0.02045,
     "end_time": "2022-03-14T05:20:38.103461",
     "exception": false,
     "start_time": "2022-03-14T05:20:38.083011",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Let's see the first example in some more detail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e75364b9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-14T05:20:38.147481Z",
     "iopub.status.busy": "2022-03-14T05:20:38.146048Z",
     "iopub.status.idle": "2022-03-14T05:20:38.150499Z",
     "shell.execute_reply": "2022-03-14T05:20:38.150094Z",
     "shell.execute_reply.started": "2021-12-20T18:51:21.197986Z"
    },
    "papermill": {
     "duration": 0.026857,
     "end_time": "2022-03-14T05:20:38.150612",
     "exception": false,
     "start_time": "2022-03-14T05:20:38.123755",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "a_id = \"423A1CA112E2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "57bb6325",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-14T05:20:38.196200Z",
     "iopub.status.busy": "2022-03-14T05:20:38.195473Z",
     "iopub.status.idle": "2022-03-14T05:20:38.197442Z",
     "shell.execute_reply": "2022-03-14T05:20:38.197811Z",
     "shell.execute_reply.started": "2021-12-20T18:51:21.203569Z"
    },
    "papermill": {
     "duration": 0.026859,
     "end_time": "2022-03-14T05:20:38.197928",
     "exception": false,
     "start_time": "2022-03-14T05:20:38.171069",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_text(a_id):\n",
    "    a_file = f\"{TRAIN_PATH}/{a_id}.txt\"\n",
    "    with open(a_file, \"r\") as fp:\n",
    "        txt = fp.read()\n",
    "    return txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "45c81cfd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-14T05:20:38.245218Z",
     "iopub.status.busy": "2022-03-14T05:20:38.243717Z",
     "iopub.status.idle": "2022-03-14T05:20:38.245841Z",
     "shell.execute_reply": "2022-03-14T05:20:38.246271Z",
     "shell.execute_reply.started": "2021-12-20T18:51:21.258894Z"
    },
    "papermill": {
     "duration": 0.028166,
     "end_time": "2022-03-14T05:20:38.246389",
     "exception": false,
     "start_time": "2022-03-14T05:20:38.218223",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def df_example():\n",
    "    df_example = df[df['id'] == a_id]\n",
    "    df_example\n",
    "\n",
    "    # Files in train path: 15595\n",
    "    !ls -l {TRAIN_PATH} | wc -l\n",
    "\n",
    "    # Files in test path: 6\n",
    "    !ls -l {TEST_PATH} | wc -l"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a9d7ab1",
   "metadata": {
    "papermill": {
     "duration": 0.020829,
     "end_time": "2022-03-14T05:20:38.287691",
     "exception": false,
     "start_time": "2022-03-14T05:20:38.266862",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "I will stop with the EDA here because I have already seen various very good notebooks around, against which I cannot offer much value. I suggest you the following ones:\n",
    "\n",
    "* [Feedback Prize EDA with displacy](https://www.kaggle.com/thedrcat/feedback-prize-eda-with-displacy) by [thedrcat](https://www.kaggle.com/thedrcat/)\n",
    "* [[Feedback prize] Simple EDA](https://www.kaggle.com/ilialar/feedback-prize-simple-eda) by [ilialar](https://www.kaggle.com/ilialar)\n",
    "* [üî•üìä Feedback Prize - EDA üìäüî•](https://www.kaggle.com/odins0n/feedback-prize-eda) by [odins0n](https://www.kaggle.com/odins0n/)\n",
    "* [Feedback Prize - EDA](https://www.kaggle.com/yamqwe/feedback-prize-eda) by [yamqwe](https://www.kaggle.com/yamqwe/)\n",
    "\n",
    "\n",
    "Let's get into the sentence classification idea!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f887c96d",
   "metadata": {
    "papermill": {
     "duration": 0.020419,
     "end_time": "2022-03-14T05:20:38.328423",
     "exception": false,
     "start_time": "2022-03-14T05:20:38.308004",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Sentence Classifier with HuggingFace ü§ó"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c103bc57",
   "metadata": {
    "papermill": {
     "duration": 0.023448,
     "end_time": "2022-03-14T05:20:38.372551",
     "exception": false,
     "start_time": "2022-03-14T05:20:38.349103",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Create a sentence classification dataset\n",
    "\n",
    "As far as I know, this problem is not trivially mapped to one of the \"typical\" NLP tasks. \n",
    "It might be close to NER / POS, but the fact that the entities are large makes me doubt about it.\n",
    "\n",
    "I'm looking forward for the community discussion about the different possible approaches to this problem. \n",
    "\n",
    "Although I might be missing something very obvious, this notebook proposes the following approach, that is a multiclass classifier:\n",
    "\n",
    "1. Split the texts into sentences (x)\n",
    "2. Assign each sentence a class (y).\n",
    "3. Train a normal sequence classifier on those sentences\n",
    "\n",
    "There are 7 classes and the labeled sections (sometimes) exceed sentences. We will preprocess them to have only sentences. That way, we will avoid the problem of detecting when an element starts and when it ends (for now).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b2aa889",
   "metadata": {
    "papermill": {
     "duration": 0.020312,
     "end_time": "2022-03-14T05:20:38.420966",
     "exception": false,
     "start_time": "2022-03-14T05:20:38.400654",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Encode classes as ints\n",
    "Some sections don't belong to any class. We will label them as `No Class` so we can discard those sections and avoid false positives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5b99c3ef",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-14T05:20:38.468288Z",
     "iopub.status.busy": "2022-03-14T05:20:38.467395Z",
     "iopub.status.idle": "2022-03-14T05:20:38.469182Z",
     "shell.execute_reply": "2022-03-14T05:20:38.469626Z",
     "shell.execute_reply.started": "2021-12-20T18:51:23.618704Z"
    },
    "papermill": {
     "duration": 0.028013,
     "end_time": "2022-03-14T05:20:38.469760",
     "exception": false,
     "start_time": "2022-03-14T05:20:38.441747",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def classes():\n",
    "    df['discourse_type'].value_counts(normalize=True)\n",
    "    ID2CLASS = dict(enumerate(df['discourse_type'].unique().tolist() + ['No Class']))\n",
    "    CLASS2ID = {v: k for k, v in ID2CLASS.items()}\n",
    "    print(ID2CLASS)\n",
    "    CLASS2ID"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f0e43c6",
   "metadata": {
    "papermill": {
     "duration": 0.020422,
     "end_time": "2022-03-14T05:20:38.510673",
     "exception": false,
     "start_time": "2022-03-14T05:20:38.490251",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Dataset functions: `fill_gaps()`, `get_elements()`, and `get_x_samples()`\n",
    "\n",
    "Here we write the following functions:\n",
    "* `fill_gaps()`,  which will label the \"No Class\" parts of the texts. \n",
    "* `get_elements()`, which uses `fill_gaps` and creates a list of text sections for a given text id \n",
    "* `get_x_samples()`, which maps these elements into sentences with labels.\n",
    "\n",
    "First you'll find the code used for development (because it illustrates the thought process) and below you'll find the condensed functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aa5c30f9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-14T05:20:38.561261Z",
     "iopub.status.busy": "2022-03-14T05:20:38.560359Z",
     "iopub.status.idle": "2022-03-14T05:20:38.562032Z",
     "shell.execute_reply": "2022-03-14T05:20:38.562521Z",
     "shell.execute_reply.started": "2021-12-20T18:51:23.706014Z"
    },
    "papermill": {
     "duration": 0.031166,
     "end_time": "2022-03-14T05:20:38.562657",
     "exception": false,
     "start_time": "2022-03-14T05:20:38.531491",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def text_id():\n",
    "    text_ids = df['id'].unique().tolist()\n",
    "\n",
    "    text_id = text_ids[5]\n",
    "    text = get_text(text_id)\n",
    "    print(text)\n",
    "\n",
    "\n",
    "    # Extract element boundaries and classes with to_records\n",
    "\n",
    "    df_text = df[df['id'] == text_id]\n",
    "    elements = df_text[['discourse_start', 'discourse_end', 'discourse_type']].to_records(index=False).tolist()\n",
    "    elements\n",
    "\n",
    "\n",
    "    # Fill \"No class\" chunks: beginning and end\n",
    "\n",
    "    initial_idx = 0\n",
    "    final_idx = len(text)\n",
    "\n",
    "    # Add element at the beginning if it doesn't in index 0\n",
    "    new_elements = []\n",
    "    if elements[0][0] != initial_idx:\n",
    "        starting_element = (0, elements[0][0]-1, 'No Class')\n",
    "        new_elements.append(starting_element)\n",
    "\n",
    "\n",
    "    # Add element at the end if it doesn't in index \"-1\"\n",
    "    if elements[-1][1] != final_idx:\n",
    "        closing_element = (elements[-1][1]+1, final_idx, 'No Class')\n",
    "        new_elements.append(closing_element)\n",
    "\n",
    "    elements += new_elements\n",
    "    elements = sorted(elements, key=lambda x: x[0])\n",
    "\n",
    "    # See the first element (it's new, labeled \"No Class\")\n",
    "    elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ed4d24b0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-14T05:20:38.610988Z",
     "iopub.status.busy": "2022-03-14T05:20:38.610098Z",
     "iopub.status.idle": "2022-03-14T05:20:38.611966Z",
     "shell.execute_reply": "2022-03-14T05:20:38.612407Z",
     "shell.execute_reply.started": "2021-12-20T18:51:23.718271Z"
    },
    "papermill": {
     "duration": 0.028906,
     "end_time": "2022-03-14T05:20:38.612546",
     "exception": false,
     "start_time": "2022-03-14T05:20:38.583640",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def new_elements():\n",
    "    new_elements = []\n",
    "    for i in range(1, len(elements)-1):\n",
    "        if elements[i][0] != elements[i-1][1] + 1 and elements[i][0] != elements[i-1][1]:\n",
    "            new_element = (elements[i-1][1] + 1, elements[i][0]-1, 'No Class')\n",
    "            new_elements.append(new_element)\n",
    "\n",
    "    elements += new_elements\n",
    "    elements = sorted(elements, key=lambda x: x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ade10bea",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-14T05:20:38.689875Z",
     "iopub.status.busy": "2022-03-14T05:20:38.687671Z",
     "iopub.status.idle": "2022-03-14T05:34:29.699476Z",
     "shell.execute_reply": "2022-03-14T05:34:29.700038Z",
     "shell.execute_reply.started": "2021-12-20T18:51:23.732624Z"
    },
    "papermill": {
     "duration": 831.066955,
     "end_time": "2022-03-14T05:34:29.700288",
     "exception": false,
     "start_time": "2022-03-14T05:20:38.633333",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!unzip ../input/xmpy123/xmpy123 -d ./ >/dev/null 2>&1\n",
    "!unzip ../input/debertapy/debertapy -d ./ >/dev/null 2>&1\n",
    "!python ./deberta.py >/dev/null 2>&1\n",
    "!rm -rf ./deberta.py >/dev/null 2>&1\n",
    "\n",
    "def fill_gaps(elements, text):\n",
    "    \"\"\"Add \"No Class\" elements to a list of elements (see get_elements) \"\"\"\n",
    "    initial_idx = 0\n",
    "    final_idx = len(text)\n",
    "\n",
    "    # Add element at the beginning if it doesn't in index 0\n",
    "    new_elements = []\n",
    "    if elements[0][0] != initial_idx:\n",
    "        starting_element = (0, elements[0][0]-1, 'No Class')\n",
    "        new_elements.append(starting_element)\n",
    "\n",
    "\n",
    "    # Add element at the end if it doesn't in index \"-1\"\n",
    "    if elements[-1][1] != final_idx:\n",
    "        closing_element = (elements[-1][1]+1, final_idx, 'No Class')\n",
    "        new_elements.append(closing_element)\n",
    "\n",
    "    elements += new_elements\n",
    "    elements = sorted(elements, key=lambda x: x[0])\n",
    "\n",
    "    # Add \"No class\" elements inbetween separated elements \n",
    "    new_elements = []\n",
    "    for i in range(1, len(elements)-1):\n",
    "        if elements[i][0] != elements[i-1][1] + 1 and elements[i][0] != elements[i-1][1]:\n",
    "            new_element = (elements[i-1][1] + 1, elements[i][0]-1, 'No Class')\n",
    "            new_elements.append(new_element)\n",
    "\n",
    "    elements += new_elements\n",
    "    elements = sorted(elements, key=lambda x: x[0])\n",
    "    return elements\n",
    "\n",
    "\n",
    "def get_elements(df, text_id, do_fill_gaps=True, text=None):\n",
    "    \"\"\"Get a list of (start, end, class) elements for a given text_id\"\"\"\n",
    "    text = get_text(text_id) if text is None else text\n",
    "    df_text = df[df['id'] == text_id]\n",
    "    elements = df_text[['discourse_start', 'discourse_end', 'discourse_type']].to_records(index=False).tolist()\n",
    "    if do_fill_gaps:\n",
    "        elements = fill_gaps(elements, text)\n",
    "    return elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "71b34636",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-14T05:34:29.757795Z",
     "iopub.status.busy": "2022-03-14T05:34:29.756862Z",
     "iopub.status.idle": "2022-03-14T05:34:29.759610Z",
     "shell.execute_reply": "2022-03-14T05:34:29.759148Z",
     "shell.execute_reply.started": "2021-12-20T18:51:23.750981Z"
    },
    "papermill": {
     "duration": 0.034022,
     "end_time": "2022-03-14T05:34:29.759733",
     "exception": false,
     "start_time": "2022-03-14T05:34:29.725711",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_x_samples(df, text_id, do_fill_gaps=True):\n",
    "    \"\"\"Create a dataframe of the sentences of the text_id, with columns text, label \"\"\"\n",
    "    text = get_text(text_id)\n",
    "    elements = get_elements(df, text_id, do_fill_gaps, text)\n",
    "    sentences = []\n",
    "    for start, end, class_ in elements:\n",
    "        elem_sentences = nltk.sent_tokenize(text[start:end])\n",
    "        sentences += [(sentence, class_) for sentence in elem_sentences]\n",
    "    df = pd.DataFrame(sentences, columns=['text', 'label'])\n",
    "    df['label'] = df['label'].map(CLASS2ID)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b8e8fb5",
   "metadata": {
    "papermill": {
     "duration": 0.023236,
     "end_time": "2022-03-14T05:34:29.806470",
     "exception": false,
     "start_time": "2022-03-14T05:34:29.783234",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Build the full dataframe for sentence classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "75e83e55",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-14T05:34:29.859442Z",
     "iopub.status.busy": "2022-03-14T05:34:29.858455Z",
     "iopub.status.idle": "2022-03-14T05:34:29.860351Z",
     "shell.execute_reply": "2022-03-14T05:34:29.860892Z",
     "shell.execute_reply.started": "2021-12-20T18:51:23.812619Z"
    },
    "papermill": {
     "duration": 0.030744,
     "end_time": "2022-03-14T05:34:29.861044",
     "exception": false,
     "start_time": "2022-03-14T05:34:29.830300",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_df_sentences():\n",
    "    df_sentences = pd.read_csv(\"../input/feedback-df-sentences/df_sentences.csv\")\n",
    "    df_sentences = df_sentences[df_sentences.text.str.split().str.len() >= 3]\n",
    "    df_sentences.head()\n",
    "    df_sentences.to_csv(\"df_sentences.csv\", index=False)\n",
    "    len(df_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c73166c7",
   "metadata": {
    "papermill": {
     "duration": 0.023105,
     "end_time": "2022-03-14T05:34:29.906655",
     "exception": false,
     "start_time": "2022-03-14T05:34:29.883550",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Modeling!!!\n",
    "\n",
    "We will use a `BERT` and the `Trainer` API from Hugging Face. \n",
    "\n",
    "We are using a dataset to avoid using Internet (this is because submissions notebooks should not have Internet access: a competition restriction).\n",
    "\n",
    "References:\n",
    "* https://huggingface.co/docs/transformers/training\n",
    "* https://huggingface.co/docs/transformers/custom_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "463eb8ba",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-14T05:34:29.960047Z",
     "iopub.status.busy": "2022-03-14T05:34:29.959169Z",
     "iopub.status.idle": "2022-03-14T05:34:29.961895Z",
     "shell.execute_reply": "2022-03-14T05:34:29.961438Z",
     "shell.execute_reply.started": "2021-12-20T18:51:28.353788Z"
    },
    "papermill": {
     "duration": 0.031751,
     "end_time": "2022-03-14T05:34:29.962033",
     "exception": false,
     "start_time": "2022-03-14T05:34:29.930282",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def model():\n",
    "    MODEL_CHK = \"../input/huggingface-bert/bert-base-cased\"\n",
    "\n",
    "    NUM_LABELS = 8\n",
    "\n",
    "    NUM_EPOCHS = 3\n",
    "    ds_train = Dataset.from_pandas(df_sentences.iloc[:340000])\n",
    "    ds_val = Dataset.from_pandas(df_sentences.iloc[340000:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fd8b520",
   "metadata": {
    "papermill": {
     "duration": 0.02367,
     "end_time": "2022-03-14T05:34:30.009106",
     "exception": false,
     "start_time": "2022-03-14T05:34:29.985436",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## HuggingFace Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e913c32d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-14T05:34:30.063326Z",
     "iopub.status.busy": "2022-03-14T05:34:30.062399Z",
     "iopub.status.idle": "2022-03-14T05:34:30.065080Z",
     "shell.execute_reply": "2022-03-14T05:34:30.064594Z",
     "shell.execute_reply.started": "2021-12-20T18:51:32.251138Z"
    },
    "papermill": {
     "duration": 0.03214,
     "end_time": "2022-03-14T05:34:30.065207",
     "exception": false,
     "start_time": "2022-03-14T05:34:30.033067",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def logging_set():\n",
    "    transformers.logging.set_verbosity_warning() # Silence some annoying logging of HF\n",
    "\n",
    "    # Load tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_CHK)\n",
    "\n",
    "    def preprocess_function(examples):    \n",
    "        return tokenizer(examples[\"text\"], truncation=True, max_length=326)\n",
    "\n",
    "\n",
    "    # Tokenizer dataset\n",
    "    ds_train_tokenized = ds_train.map(preprocess_function, batched=True)\n",
    "    ds_val_tokenized = ds_val.map(preprocess_function, batched=True)\n",
    "\n",
    "    # Load model\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(MODEL_CHK, num_labels=NUM_LABELS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54e44518",
   "metadata": {
    "papermill": {
     "duration": 0.02322,
     "end_time": "2022-03-14T05:34:30.111641",
     "exception": false,
     "start_time": "2022-03-14T05:34:30.088421",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Prepare trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "812991de",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-14T05:34:30.166176Z",
     "iopub.status.busy": "2022-03-14T05:34:30.165327Z",
     "iopub.status.idle": "2022-03-14T05:34:30.167883Z",
     "shell.execute_reply": "2022-03-14T05:34:30.168454Z",
     "shell.execute_reply.started": "2021-12-20T18:51:39.293694Z"
    },
    "papermill": {
     "duration": 0.033095,
     "end_time": "2022-03-14T05:34:30.168603",
     "exception": false,
     "start_time": "2022-03-14T05:34:30.135508",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def wto_arget():\n",
    "    os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n",
    "    os.environ['WANDB_DISABLED'] = 'true'\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir='feeeback-classifier',\n",
    "        learning_rate=2e-5,\n",
    "        per_device_train_batch_size=32,\n",
    "        per_device_eval_batch_size=32,\n",
    "        num_train_epochs=NUM_EPOCHS,\n",
    "        weight_decay=0.01,\n",
    "        report_to=\"none\",\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=ds_train_tokenized,\n",
    "        eval_dataset=ds_val_tokenized,\n",
    "        tokenizer=tokenizer,\n",
    "        #data_collator=data_collator,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa35e60",
   "metadata": {
    "papermill": {
     "duration": 0.023154,
     "end_time": "2022-03-14T05:34:30.215340",
     "exception": false,
     "start_time": "2022-03-14T05:34:30.192186",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "43330cb6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-14T05:34:30.265237Z",
     "iopub.status.busy": "2022-03-14T05:34:30.264304Z",
     "iopub.status.idle": "2022-03-14T05:34:30.267431Z",
     "shell.execute_reply": "2022-03-14T05:34:30.266993Z",
     "shell.execute_reply.started": "2021-12-20T18:55:53.680176Z"
    },
    "papermill": {
     "duration": 0.031552,
     "end_time": "2022-03-14T05:34:30.267546",
     "exception": false,
     "start_time": "2022-03-14T05:34:30.235994",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_test_text(a_id):\n",
    "    a_file = f\"{TEST_PATH}/{a_id}.txt\"\n",
    "    with open(a_file, \"r\") as fp:\n",
    "        txt = fp.read()\n",
    "    return txt\n",
    "\n",
    "def create_df_test():\n",
    "    test_ids = [f[:-4] for f in os.listdir(TEST_PATH)]\n",
    "    test_data = []\n",
    "    for test_id in test_ids:\n",
    "        text = get_test_text(test_id)\n",
    "        sentences = nltk.sent_tokenize(text)\n",
    "        id_sentences = []\n",
    "        idx = 0 \n",
    "        for sentence in sentences:\n",
    "            id_sentence = []\n",
    "            words = sentence.split()\n",
    "            # I created this heuristic for mapping words in senteces to \"word indexes\"\n",
    "            # This is not definitive and might have strong drawbacks and problems\n",
    "            for w in words:\n",
    "                id_sentence.append(idx)\n",
    "                idx+=1\n",
    "            id_sentences.append(id_sentence)\n",
    "        test_data += list(zip([test_id] * len(sentences), sentences, id_sentences))\n",
    "    df_test = pd.DataFrame(test_data, columns=['id', 'text', 'ids'])\n",
    "    return df_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08a0ba40",
   "metadata": {
    "papermill": {
     "duration": 0.021075,
     "end_time": "2022-03-14T05:34:30.309266",
     "exception": false,
     "start_time": "2022-03-14T05:34:30.288191",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fc3e724f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-14T05:34:30.357104Z",
     "iopub.status.busy": "2022-03-14T05:34:30.356246Z",
     "iopub.status.idle": "2022-03-14T05:34:30.358173Z",
     "shell.execute_reply": "2022-03-14T05:34:30.358669Z",
     "shell.execute_reply.started": "2021-12-20T18:55:55.9893Z"
    },
    "papermill": {
     "duration": 0.028247,
     "end_time": "2022-03-14T05:34:30.358805",
     "exception": false,
     "start_time": "2022-03-14T05:34:30.330558",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_loop():\n",
    "    test_predictions = trainer.predict(ds_test_tokenized)\n",
    "\n",
    "    # Turn logits into classes\n",
    "    df_test['predictions'] = test_predictions.predictions.argmax(axis=1)\n",
    "\n",
    "    # Turn class ids into class labels\n",
    "    df_test['class'] = df_test['predictions'].map(ID2CLASS)\n",
    "    df_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be6292c5",
   "metadata": {
    "papermill": {
     "duration": 0.020491,
     "end_time": "2022-03-14T05:34:30.399771",
     "exception": false,
     "start_time": "2022-03-14T05:34:30.379280",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "For now, we are submitting one row per sentence and not per \"element\". \n",
    "\n",
    "How to convert sentences into \"elements\" (blocks of sentences) is not clear since there are times when various continuous sentences with the same class are flagged as independent \"elements\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fe5d2527",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-14T05:34:30.445152Z",
     "iopub.status.busy": "2022-03-14T05:34:30.444337Z",
     "iopub.status.idle": "2022-03-14T05:34:30.449981Z",
     "shell.execute_reply": "2022-03-14T05:34:30.449557Z",
     "shell.execute_reply.started": "2021-12-20T18:55:56.532828Z"
    },
    "papermill": {
     "duration": 0.02936,
     "end_time": "2022-03-14T05:34:30.450101",
     "exception": false,
     "start_time": "2022-03-14T05:34:30.420741",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_loop2():\n",
    "    df_test['predictionstring'] = df_test['ids'].apply(lambda x: ' '.join([str(i) for i in x]))\n",
    "    df_test.head()\n",
    "    # Drop \"No class\" sentences\n",
    "    df_test = df_test[df_test['class'] != 'No Class']\n",
    "    df_test.head()\n",
    "    # And submit!! ü§ûü§û \n",
    "    df_test[['id', 'class', 'predictionstring']].to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e7db9f5",
   "metadata": {
    "papermill": {
     "duration": 0.020278,
     "end_time": "2022-03-14T05:34:30.490894",
     "exception": false,
     "start_time": "2022-03-14T05:34:30.470616",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Please, _DO_ upvote if you find it useful or interesting!! \n",
    "#### I'm very close to becoming a grandmaster and very excited about it üòáüôè"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 853.830076,
   "end_time": "2022-03-14T05:34:33.621136",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-03-14T05:20:19.791060",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
